{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    root_mean_squared_error,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç 1Ô∏è‚É£ Exploratory Data Analysis (EDA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test datasets\n",
    "df = pd.read_csv(\"dataset/train.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First rows\", df.head(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Info\", df.info(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Handle Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "df[numerical_features] = df[numerical_features].fillna(df[numerical_features].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = df.select_dtypes(include=[\"object\"]).columns\n",
    "df[categorical_features] = df[categorical_features].fillna(\"MISSING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Summary Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Visualizing Target Variable (`SalePrice`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "categorical_features = df.select_dtypes(include=[\"object\"]).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Distribution of Numerical Features with SalePrice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    nrows=len(numerical_features) // 4 + 1,\n",
    "    ncols=4,\n",
    "    figsize=(20, len(numerical_features) * 1.2),\n",
    ")\n",
    "fig.suptitle(\"Distribution of Numerical Features with SalePrice\", fontsize=16)\n",
    "\n",
    "axes = axes.flatten()  # Flatten the 2D array for easier iteration\n",
    "\n",
    "for i, col in enumerate(numerical_features):\n",
    "    sns.histplot(df[col], kde=True, ax=axes[i], color=\"royalblue\")\n",
    "    axes[i].set_title(f\"{col} Distribution\")\n",
    "\n",
    "# Hide empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SalePrice Distribution Across Categorical Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    nrows=len(categorical_features) // 3 + 1,\n",
    "    ncols=3,\n",
    "    figsize=(20, len(categorical_features) * 1.5),\n",
    ")\n",
    "fig.suptitle(\"SalePrice Distribution Across Categorical Features\", fontsize=16)\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(categorical_features):\n",
    "    sns.boxplot(x=df[col], y=df[\"SalePrice\"], ax=axes[i])\n",
    "    axes[i].set_title(f\"{col} vs SalePrice\")\n",
    "    axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=45)\n",
    "\n",
    "# Hide empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Scatterplots of Numerical Features vs. SalePrice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    nrows=len(numerical_features) // 3 + 1,\n",
    "    ncols=3,\n",
    "    figsize=(20, len(numerical_features) * 1.5),\n",
    ")\n",
    "fig.suptitle(\"Scatterplots of Numerical Features vs. SalePrice\", fontsize=16)\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numerical_features):\n",
    "    sns.scatterplot(\n",
    "        x=df[col], y=df[\"SalePrice\"], alpha=0.6, color=\"darkorange\", ax=axes[i]\n",
    "    )\n",
    "    axes[i].set_title(f\"{col} vs SalePrice\")\n",
    "\n",
    "# Hide empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Correlation Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr_matrix = df.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 10))\n",
    "sns.heatmap(corr_matrix, cmap=\"coolwarm\", annot=True, fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix Heatmap\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå 2Ô∏è‚É£ Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Feature Enginneering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Older houses may have different pricing patterns than newer ones.\n",
    "# üè† Creating House Age Feature\n",
    "df[\"HouseAge\"] = df[\"YrSold\"] - df[\"YearBuilt\"]\n",
    "\n",
    "# üî® Remodeling Indicator\n",
    "# Some houses are renovated, affecting their value.\n",
    "df[\"WasRemodeled\"] = (df[\"YearRemodAdd\"] != df[\"YearBuilt\"]).astype(int)\n",
    "\n",
    "# üìè Total Square Footage\n",
    "# Combining basement, first, and second-floor areas into a single feature.\n",
    "df[\"TotalSF\"] = df[\"TotalBsmtSF\"] + df[\"1stFlrSF\"] + df[\"2ndFlrSF\"]\n",
    "\n",
    "df[\"LotQual\"] = df[\"LotArea\"] * df[\"OverallQual\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Encoding Categorical Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some categorical variables have a natural order (e.g., quality ratings).\n",
    "quality_features = [\"ExterQual\", \"ExterCond\", \"HeatingQC\", \"KitchenQual\"]\n",
    "\n",
    "bsmt_features = [\n",
    "    \"BsmtQual\",\n",
    "    \"BsmtCond\",\n",
    "    \"FireplaceQu\",\n",
    "    \"GarageQual\",\n",
    "    \"GarageCond\",\n",
    "]\n",
    "\n",
    "# Mapping pour chaque feature\n",
    "quality_mapping = {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n",
    "bsmt_mapping = {\"MISSING\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n",
    "\n",
    "# Appliquer le mapping\n",
    "for col in quality_features:\n",
    "    df[col] = df[col].map(quality_mapping)\n",
    "\n",
    "for col in bsmt_features:\n",
    "    df[col] = df[col].map(bsmt_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For categorical features without any ranking, we use one-hot encoding.\n",
    "categorical_features = df.select_dtypes(include=[\"object\"]).columns\n",
    "df = pd.get_dummies(df, columns=categorical_features, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 3Ô∏è‚É£ Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Step 2: Feature Importance using XGBoost\n",
    "# *******************************************\n",
    "\n",
    "# X = df.drop(columns=[\"SalePrice\"])  # Features\n",
    "# y = df[\"SalePrice\"]  # Target\n",
    "\n",
    "# # Train an XGBoost model\n",
    "# xgb = XGBRegressor(n_estimators=100, random_state=42)\n",
    "# xgb.fit(X, y)\n",
    "\n",
    "# # Get feature importance\n",
    "# xgb_importance = xgb.feature_importances_\n",
    "# xgb_importance_df = pd.DataFrame({\"Feature\": X.columns, \"Importance\": xgb_importance})\n",
    "# xgb_importance_df.sort_values(by=\"Importance\", ascending=False, inplace=True)\n",
    "\n",
    "# # Plot top 20 important features\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.barplot(x=\"Importance\", y=\"Feature\", data=xgb_importance_df[:20])\n",
    "# plt.title(\"Top 20 Important Features (XGBoost)\")\n",
    "# plt.show()\n",
    "\n",
    "# # Drop low-importance features\n",
    "# threshold = 0.005\n",
    "# selected_features_xgb = xgb_importance_df[xgb_importance_df[\"Importance\"] > threshold][\n",
    "#     \"Feature\"\n",
    "# ].tolist()\n",
    "\n",
    "# print(f\"Selected features (XGBoost): {selected_features_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Step 3: Keep only the selected features in the dataset\n",
    "# ************************************************\n",
    "\n",
    "# df = df[selected_features_xgb + [\"SalePrice\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ 4Ô∏è‚É£ Model Training & Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìå Step 1: Train-Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target variable (y)\n",
    "# df\n",
    "X = df.drop(columns=[\"SalePrice\"])\n",
    "y = df[\"SalePrice\"]\n",
    "\n",
    "# Split into training (80%) and testing (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training Set: {X_train.shape}, Testing Set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìå Step 2: Standardization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìå Step 3: Choose & Train Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models with default parameters\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(\n",
    "        n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Train models and evaluate\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    results[name] = {\n",
    "        \"R¬≤ Score\": r2_score(y_test, y_pred),\n",
    "        \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "        \"RMSE\": root_mean_squared_error(y_test, y_pred),\n",
    "    }\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for XGBoost tuning\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 250, 300, 500],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"subsample\": [0.7, 0.8, 0.9],\n",
    "    \"colsample_bytree\": [0.7, 0.8, 0.9],\n",
    "}\n",
    "\n",
    "# Grid Search for best parameters\n",
    "xgb_tuned = GridSearchCV(\n",
    "    XGBRegressor(random_state=42), param_grid, cv=3, scoring=\"r2\", n_jobs=-1, verbose=2\n",
    ")\n",
    "xgb_tuned.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"üöÄ Best XGBoost Parameters:\", xgb_tuned.best_params_)\n",
    "\n",
    "# Train optimized model\n",
    "best_xgb = XGBRegressor(**xgb_tuned.best_params_, random_state=42)\n",
    "best_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_best_xgb = best_xgb.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "xgb_r2_best = r2_score(y_test, y_pred_best_xgb)\n",
    "xgb_mae_best = mean_absolute_error(y_test, y_pred_best_xgb)\n",
    "xgb_rmse_best = root_mean_squared_error(y_test, y_pred_best_xgb)\n",
    "\n",
    "print(\n",
    "    f\"‚úÖ Optimized XGBoost Results: R¬≤ = {xgb_r2_best:.4f}, MAE = {xgb_mae_best:.4f}, RMSE = {xgb_rmse_best:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define parameter grid\n",
    "# rf_param_grid = {\n",
    "#     \"n_estimators\": [100, 120, 150, 180, 200],  # Number of trees\n",
    "#     \"max_depth\": [10, 20, None],  # Tree depth\n",
    "#     \"min_samples_split\": [2, 5, 10],  # Minimum samples to split\n",
    "#     \"min_samples_leaf\": [1, 2, 4],  # Minimum samples per leaf\n",
    "#     \"bootstrap\": [True, False],  # Bootstrapping technique\n",
    "# }\n",
    "\n",
    "# # Initialize RandomForestRegressor\n",
    "# rf_model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# # Grid search with cross-validation\n",
    "# rf_grid_search = GridSearchCV(\n",
    "#     rf_model,\n",
    "#     rf_param_grid,\n",
    "#     cv=5,\n",
    "#     scoring=\"r2\",\n",
    "#     n_jobs=-1,\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best model\n",
    "# best_rf = rf_grid_search.best_estimator_\n",
    "\n",
    "# # Print best parameters\n",
    "# print(\"üî• Best Random Forest Parameters:\", rf_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_pred = best_rf.predict(X_test)\n",
    "# xgb_pred = best_xgb.predict(X_test)\n",
    "\n",
    "# # Weighted average (adjust weights based on R¬≤ performance)\n",
    "# final_pred = (0.6 * xgb_pred) + (0.4 * rf_pred)\n",
    "\n",
    "# # Compute metrics\n",
    "# ensemble_r2 = r2_score(y_test, final_pred)\n",
    "# ensemble_mae = mean_absolute_error(y_test, final_pred)\n",
    "# ensemble_rmse = root_mean_squared_error(y_test, final_pred)\n",
    "\n",
    "# # Print results\n",
    "# print(\"‚ö° Ensemble Model Performance:\")\n",
    "# print(f\"   üìå R¬≤ Score: {ensemble_r2:.4f}\")\n",
    "# print(f\"   üìå MAE: {ensemble_mae:.2f}\")\n",
    "# print(f\"   üìå RMSE: {ensemble_rmse:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
